{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export ArcGIS Metadata as InPort XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working . . .\n",
      "Dataset: AbaloneBlack_20250521 (Count 1 of 150)\n",
      "Dataset: AbaloneWhite_20250529 (Count 2 of 150)\n",
      "Dataset: All_Species_Ranges_20250710 (Count 3 of 150)\n",
      "Dataset: AngelsharkArgentine_20250609 (Count 4 of 150)\n",
      "Dataset: AngelsharkCommon_20250609 (Count 5 of 150)\n",
      "Dataset: AngelsharkSawback_20250609 (Count 6 of 150)\n",
      "Dataset: AngelsharkSmoothback_20250609 (Count 7 of 150)\n",
      "Dataset: AngelsharkSpiny_20250609 (Count 8 of 150)\n",
      "Dataset: Bocaccio_PugetSoundGeorgiaBasinDPS_20250519 (Count 9 of 150)\n",
      "Dataset: CardinalfishBanggai_20250626 (Count 10 of 150)\n",
      "Dataset: CoelacanthAfricanTanzanianDPS_20250528 (Count 11 of 150)\n",
      "Dataset: ConchQueen_20250603 (Count 12 of 150)\n",
      "Dataset: CoralAcroporaGlobiceps_20250521 (Count 13 of 150)\n",
      "Dataset: CoralAcroporaJacquelineae_20250625 (Count 14 of 150)\n",
      "Dataset: CoralAcroporaLokani_20250625 (Count 15 of 150)\n",
      "Dataset: CoralAcroporaPharaonis_20250625 (Count 16 of 150)\n",
      "Dataset: CoralAcroporaRetusa_20250521 (Count 17 of 150)\n",
      "Dataset: CoralAcroporaRudis_20250625 (Count 18 of 150)\n",
      "Dataset: CoralAcroporaSpeciosa_20250521 (Count 19 of 150)\n",
      "Dataset: CoralAcroporaTenella_20250625 (Count 20 of 150)\n",
      "Dataset: CoralAnacroporaSpinosa_20250625 (Count 21 of 150)\n",
      "Dataset: CoralBoulderStar_20250521 (Count 22 of 150)\n",
      "Dataset: CoralCantharellusnoumeae_20250625 (Count 23 of 150)\n",
      "Dataset: CoralElkhorn_20250529 (Count 24 of 150)\n",
      "Dataset: CoralFimbriaphylliaParadivisa_20250529 (Count 25 of 150)\n",
      "Dataset: CoralIsoporaCrateriformis_20250529 (Count 26 of 150)\n",
      "Dataset: CoralLobedStar_20250529 (Count 27 of 150)\n",
      "Dataset: CoralMontiporaAustraliensis_20250625 (Count 28 of 150)\n",
      "Dataset: CoralMountainousStar_20250529 (Count 29 of 150)\n",
      "Dataset: CoralPavonaDiffluens_20250625 (Count 30 of 150)\n",
      "Dataset: CoralPillar_20250603 (Count 31 of 150)\n",
      "Dataset: CoralPoritesNapopora_20250625 (Count 32 of 150)\n",
      "Dataset: CoralRoughCactus_20250603 (Count 33 of 150)\n",
      "Dataset: CoralSeriatoporaAculeata_20250625 (Count 34 of 150)\n",
      "Dataset: CoralStaghorn_20250603 (Count 35 of 150)\n",
      "Dataset: CoralTubastraeaFloreana_20250625 (Count 36 of 150)\n",
      "Dataset: DolphinAtlanticHumpback_20250626 (Count 37 of 150)\n",
      "Dataset: DolphinChineseRiver_20250528 (Count 38 of 150)\n",
      "Dataset: DolphinHectors_20250610 (Count 39 of 150)\n",
      "Dataset: DolphinIndusRiver_20250610 (Count 40 of 150)\n",
      "Dataset: DolphinTaiwaneseHumpback_20250626 (Count 41 of 150)\n",
      "Dataset: Eulachon_SouthernDPS_20250522 (Count 42 of 150)\n",
      "Dataset: GrouperGulf_20250522 (Count 43 of 150)\n",
      "Dataset: GrouperIsland_20250627 (Count 44 of 150)\n",
      "Dataset: GrouperNassau_20250522 (Count 45 of 150)\n",
      "Dataset: GuitarfishBlackchin_20250611 (Count 46 of 150)\n",
      "Dataset: GuitarfishBrazilian_20250611 (Count 47 of 150)\n",
      "Dataset: GuitarfishCommon_20250611 (Count 48 of 150)\n",
      "Dataset: NautilusChambered_20250521 (Count 49 of 150)\n",
      "Dataset: PorpoiseGulfofCaliforniaHarborVaquita_20250626 (Count 50 of 150)\n",
      "Dataset: Proposed_ClamDevil_20250626 (Count 51 of 150)\n",
      "Dataset: Proposed_ClamHorsesHoof_20250609 (Count 52 of 150)\n",
      "Dataset: Proposed_ClamPorcelain_20250626 (Count 53 of 150)\n",
      "Dataset: Proposed_ClamRedSeaGiant_20250626 (Count 54 of 150)\n",
      "Dataset: Proposed_ClamSmoothGiant_20250609 (Count 55 of 150)\n",
      "Dataset: Proposed_ClamTrueGiant_20250609 (Count 56 of 150)\n",
      "Dataset: Proposed_SeaStarSunflower_20250603 (Count 57 of 150)\n",
      "Dataset: RayGiantManta_20250522 (Count 58 of 150)\n",
      "Dataset: RockfishYelloweye_PugetSoundGeorgiaBasinDPS_20250529 (Count 59 of 150)\n",
      "Dataset: SalmonAtlantic_GulfofMaineDPS_20250617 (Count 60 of 150)\n",
      "Dataset: SalmonChinook_AllESU_20250613_A (Count 61 of 150)\n",
      "Dataset: SalmonChinook_AllESU_20250613_B (Count 62 of 150)\n",
      "Dataset: SalmonChinook_CaliforniaCoastalESU_20250613 (Count 63 of 150)\n",
      "Dataset: SalmonChinook_CentralValleySpringRunESU_20250613 (Count 64 of 150)\n",
      "Dataset: SalmonChinook_LowerColumbiaRiverESU_20250613 (Count 65 of 150)\n",
      "Dataset: SalmonChinook_PugetSoundESU_20250613 (Count 66 of 150)\n",
      "Dataset: SalmonChinook_SacramentoRiverWinterRunESU_20250613 (Count 67 of 150)\n",
      "Dataset: SalmonChinook_SnakeRiverFallRunESU_20250613 (Count 68 of 150)\n",
      "Dataset: SalmonChinook_SnakeRiverSpringSummerRunESU_20250613 (Count 69 of 150)\n",
      "Dataset: SalmonChinook_UpperColumbiaRiverSpringRunESU_20250613 (Count 70 of 150)\n",
      "Dataset: SalmonChinook_UpperWillametteRiverESU_20250613 (Count 71 of 150)\n",
      "Dataset: SalmonChum_AllESU_20250616_A (Count 72 of 150)\n",
      "Dataset: SalmonChum_AllESU_20250616_B (Count 73 of 150)\n",
      "Dataset: SalmonChum_ColumbiaRiverESU_20250616 (Count 74 of 150)\n",
      "Dataset: SalmonChum_HoodCanalSummerRunESU_20250616 (Count 75 of 150)\n",
      "Dataset: SalmonCoho_AllESU_20250617_A (Count 76 of 150)\n",
      "Dataset: SalmonCoho_AllESU_20250617_B (Count 77 of 150)\n",
      "Dataset: SalmonCoho_CentralCaliforniaCoastESU_20250617 (Count 78 of 150)\n",
      "Dataset: SalmonCoho_LowerColumbiaRiverESU_20250617 (Count 79 of 150)\n",
      "Dataset: SalmonCoho_OregonCoastESU_20250617 (Count 80 of 150)\n",
      "Dataset: SalmonCoho_SouthernOregonNorthernCaliforniaCoastESU_20250617 (Count 81 of 150)\n",
      "Dataset: SalmonSockeye_AllESU_20250617_A (Count 82 of 150)\n",
      "Dataset: SalmonSockeye_AllESU_20250617_B (Count 83 of 150)\n",
      "Dataset: SalmonSockeye_OzetteLakeESU_20250617 (Count 84 of 150)\n",
      "Dataset: SalmonSockeye_SnakeRiverESU_20250617 (Count 85 of 150)\n",
      "Dataset: SawfishDwarf_20250627 (Count 86 of 150)\n",
      "Dataset: SawfishGreen_20250627 (Count 87 of 150)\n",
      "Dataset: SawfishLargetooth_20250627 (Count 88 of 150)\n",
      "Dataset: SawfishNarrow_20250627 (Count 89 of 150)\n",
      "Dataset: SawfishSmalltooth_AllDPS_20250528 (Count 90 of 150)\n",
      "Dataset: SeaLionSteller_WesternDPS_20250611 (Count 91 of 150)\n",
      "Dataset: SeaSnakeDusky_20250627 (Count 92 of 150)\n",
      "Dataset: SeaTurtleGreen_AllDPS_20250609 (Count 93 of 150)\n",
      "Dataset: SeaTurtleHawksbill_20250529 (Count 94 of 150)\n",
      "Dataset: SeaTurtleKempsRidley_20250529 (Count 95 of 150)\n",
      "Dataset: SeaTurtleLeatherback_20250529 (Count 96 of 150)\n",
      "Dataset: SeaTurtleLoggerhead_AllDPS_20250609 (Count 97 of 150)\n",
      "Dataset: SeaTurtleOliveRidley_All_20250609 (Count 98 of 150)\n",
      "Dataset: SealBearded_AllDPS_20250611 (Count 99 of 150)\n",
      "Dataset: SealGuadalupeFur_20250611 (Count 100 of 150)\n",
      "Dataset: SealHawaiianMonk_20250611 (Count 101 of 150)\n",
      "Dataset: SealMediterraneanMonk_20250626 (Count 102 of 150)\n",
      "Dataset: SealRinged_AllSubspecies_20250612 (Count 103 of 150)\n",
      "Dataset: SealSpotted_SouthernDPS_20250611 (Count 104 of 150)\n",
      "Dataset: SharkDaggernose_20250610 (Count 105 of 150)\n",
      "Dataset: SharkNarrownoseSmoothhound_20250610 (Count 106 of 150)\n",
      "Dataset: SharkOceanicWhitetip_20250529 (Count 107 of 150)\n",
      "Dataset: SharkScallopedHammerhead_AllDPS_20250609 (Count 108 of 150)\n",
      "Dataset: SharkStripedSmoothhound_20250610 (Count 109 of 150)\n",
      "Dataset: Steelhead_AllDPS_20250617_A (Count 110 of 150)\n",
      "Dataset: Steelhead_AllDPS_20250617_B (Count 111 of 150)\n",
      "Dataset: Steelhead_CaliforniaCentralValleyDPS_20250617 (Count 112 of 150)\n",
      "Dataset: Steelhead_CentralCaliforniaCoastDPS_20250617 (Count 113 of 150)\n",
      "Dataset: Steelhead_LowerColumbiaRiverDPS_20250617 (Count 114 of 150)\n",
      "Dataset: Steelhead_MiddleColumbiaRiverDPS_20250617 (Count 115 of 150)\n",
      "Dataset: Steelhead_NorthernCaliforniaDPS_20250617 (Count 116 of 150)\n",
      "Dataset: Steelhead_PugetSoundDPS_20250617 (Count 117 of 150)\n",
      "Dataset: Steelhead_SnakeRiverBasinDPS_20250617 (Count 118 of 150)\n",
      "Dataset: Steelhead_SouthCentralCaliforniaCoastDPS_20250617 (Count 119 of 150)\n",
      "Dataset: Steelhead_SouthernCaliforniaDPS_20250617 (Count 120 of 150)\n",
      "Dataset: Steelhead_UpperColumbiaRiverDPS_20250617 (Count 121 of 150)\n",
      "Dataset: Steelhead_UpperWillametteRiverDPS_20250617 (Count 122 of 150)\n",
      "Dataset: SturgeonAdriatic_20250627 (Count 123 of 150)\n",
      "Dataset: SturgeonAtlantic_CarolinaDPS_20250620 (Count 124 of 150)\n",
      "Dataset: SturgeonAtlantic_ChesapeakeBayDPS_20250620 (Count 125 of 150)\n",
      "Dataset: SturgeonAtlantic_GulfofMaineDPS_20250620 (Count 126 of 150)\n",
      "Dataset: SturgeonAtlantic_NewYorkBightDPS_20250620 (Count 127 of 150)\n",
      "Dataset: SturgeonAtlantic_SouthAtlanticDPS_20250620 (Count 128 of 150)\n",
      "Dataset: SturgeonChinese_20250627 (Count 129 of 150)\n",
      "Dataset: SturgeonEuropean_20250627 (Count 130 of 150)\n",
      "Dataset: SturgeonGreen_SouthernDPS_20250618 (Count 131 of 150)\n",
      "Dataset: SturgeonGulf_20250626 (Count 132 of 150)\n",
      "Dataset: SturgeonKaluga_20250627 (Count 133 of 150)\n",
      "Dataset: SturgeonSakhalin_20250627 (Count 134 of 150)\n",
      "Dataset: SturgeonShortnose_20250623 (Count 135 of 150)\n",
      "Dataset: TotoabaSeatroutWeakfish_20250626 (Count 136 of 150)\n",
      "Dataset: WhaleBeluga_CookInletDPS_20250624 (Count 137 of 150)\n",
      "Dataset: WhaleBlue_20250627 (Count 138 of 150)\n",
      "Dataset: WhaleBowhead_20250627 (Count 139 of 150)\n",
      "Dataset: WhaleFalseKiller_MainHawaiianIslandsInsularDPS_20250624 (Count 140 of 150)\n",
      "Dataset: WhaleFinback_20250627 (Count 141 of 150)\n",
      "Dataset: WhaleGray_WesternNorthPacificDPS_20250627 (Count 142 of 150)\n",
      "Dataset: WhaleHumpback_AllDPS_20250627 (Count 143 of 150)\n",
      "Dataset: WhaleKiller_SouthernResidentDPS_20250624 (Count 144 of 150)\n",
      "Dataset: WhaleNorthAtlanticRight_20250627 (Count 145 of 150)\n",
      "Dataset: WhaleNorthPacificRight_20250630 (Count 146 of 150)\n",
      "Dataset: WhaleRices_20250630 (Count 147 of 150)\n",
      "Dataset: WhaleSei_20250630 (Count 148 of 150)\n",
      "Dataset: WhaleSouthernRight_20250626 (Count 149 of 150)\n",
      "Dataset: WhaleSperm_20250630 (Count 150 of 150)\n",
      "Work Complete\n"
     ]
    }
   ],
   "source": [
    "import os, traceback\n",
    "import arcpy\n",
    "from arcpy import metadata as md\n",
    "from lxml import etree\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# Get the Home Folder and the Default Geodatabase from the CURRENT ArcGIS Pro project\n",
    "home_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "project_gdb = arcpy.mp.ArcGISProject(\"CURRENT\").defaultGeodatabase\n",
    "\n",
    "export_folder = f\"{home_folder}\\Metadata Export\"\n",
    "if not arcpy.Exists(export_folder):\n",
    "    arcpy.management.CreateFolder(home_folder, \"Metadata Export\")\n",
    "del export_folder\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = project_gdb\n",
    "\n",
    "# Reference the XSL file to use for the transformation\n",
    "xsl_file  = rf\"{home_folder}\\ArcGIS2Inport.xsl\"\n",
    "    \n",
    "datasets = list()\n",
    "\n",
    "walk = arcpy.da.Walk(project_gdb)\n",
    "\n",
    "for dirpath, dirnames, filenames in walk:\n",
    "    for filename in filenames:\n",
    "        datasets.append(os.path.join(dirpath, filename))\n",
    "\n",
    "if len(datasets) == 0:\n",
    "    print(\"No datasets found.\")\n",
    "elif len(datasets) > 0:\n",
    "    print(\"Working . . .\")\n",
    "    count=0\n",
    "    for dataset in sorted(datasets):\n",
    "        count+=1\n",
    "        print(f\"Dataset: {os.path.basename(dataset)} (Count {count} of {len(datasets)})\")        \n",
    "        dataset_md = md.Metadata(dataset)\n",
    "        #dataset_md.synchronize(\"ALWAYS\")\n",
    "        #dataset_md.save()\n",
    "        #print(f\"{home_folder}\\Metadata Export\\{os.path.basename(dataset)}_InPort.xml\")\n",
    "\n",
    "        tree = etree.parse(StringIO(dataset_md.xml), parser=etree.XMLParser(encoding='UTF-8', remove_blank_text=True))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        mdParentID = root.find(\"mdParentID\")  # Prod: 65940 Test: 75873, prod dismap 66799\n",
    "        \n",
    "        if mdParentID is None:\n",
    "            _xml = etree.XML(f\"<mdParentID>gov.noaa.nmfs.inport:65940</mdParentID>\", etree.XMLParser(encoding='UTF-8', remove_blank_text=True))\n",
    "            root.insert(root_dict[\"mdParentID\"], _xml)\n",
    "            del _xml\n",
    "        elif mdParentID is not None:\n",
    "            mdParentID.text = \"gov.noaa.nmfs.inport:65940\"\n",
    "        else:\n",
    "            none            \n",
    "        #print(mdParentID.text)\n",
    "        \n",
    "        del mdParentID\n",
    "\n",
    "        etree.indent(tree, \"    \")\n",
    "        dataset_md.xml = etree.tostring(tree, encoding='UTF-8', method='xml', xml_declaration=True, pretty_print=True).decode()\n",
    "        dataset_md.save()\n",
    "        dataset_md.synchronize(\"ALWAYS\")\n",
    "        dataset_md.save()\n",
    "        \n",
    "        try:\n",
    "            dataset_md.exportMetadata(f\"{home_folder}\\Metadata Export\\{os.path.basename(dataset)}_InPort.xml\", \"CUSTOM\", \"EXACT_COPY\", xsl_file)\n",
    "        except arcpy.ExecuteError:\n",
    "            print(arcpy.GetMessages())\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            arcpy.GetMessages()\n",
    "        del dataset_md\n",
    "        del dataset\n",
    "    print(\"Work Complete\")\n",
    "else:\n",
    "    pass\n",
    "del datasets\n",
    "del etree, md, os, StringIO\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coded\n",
    "credentials={\"username\" : \"firstname.lastname@noaa.gov\", \"password\" : \"NOAA_LDAP_PASSWORD\"}\n",
    "# OR\n",
    "# Project (Home) Folder from currently opened ArcGIS Pro\n",
    "home_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "# Load user_credentials.py to get user credentials\n",
    "# def main():\n",
    "#     credentials={\"username\" : \"firstname.lastname@noaa.gov\", \"password\" : \"NOAA_LDAP_PASSWORD\"}\n",
    "#     return credentials\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "# Imports\n",
    "import os\n",
    "# Change Directory\n",
    "os.chdir(home_folder)\n",
    "import user_credentials\n",
    "credentials = user_credentials.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contacts API Specs ###\n",
    "### Endpoints ###\n",
    "#### /inport/api/find-contact ####\n",
    "#### (POST) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n",
      "{'sessionId': '7340dbd8e69622674f4bbcce'}\n",
      "results\n",
      "\tlastName: Schultz\n",
      "\tfirstName: Jennifer\n",
      "\temailAddress: jennifer.schultz@noaa.gov\n",
      "\tcontactType: Person\n",
      "\torcIdMaxLength: 19\n",
      "\trorMaxLength: 9\n"
     ]
    }
   ],
   "source": [
    "import requests, json, copy\n",
    "inport_url = \"https://www.fisheries.noaa.gov/inport\"\n",
    "inport_response = requests.get(url=inport_url, auth=(credentials[\"username\"], credentials[\"password\"]))\n",
    "if inport_response.status_code == 200:\n",
    "    print(\"Authentication successful!\")\n",
    "    _url = \"https://www.fisheries.noaa.gov/inport/api/get-session-id\"\n",
    "    session_id_response = requests.post(_url, json = credentials)\n",
    "    session_id_dict = copy.deepcopy(session_id_response.json())\n",
    "    print(session_id_dict)\n",
    "    session_id_response.close()\n",
    "    del _url, session_id_response    \n",
    "    \n",
    "    session_id_dict[\"contactType\"]  = \"Person\"\n",
    "    session_id_dict[\"emailAddress\"] = \"jennifer.schultz@noaa.gov\"\n",
    "    \n",
    "    _url = \"https://www.fisheries.noaa.gov/inport/api/find-contact\"\n",
    "    response = requests.get(_url, params = session_id_dict)        \n",
    "    #print(response.json())\n",
    "    _dict = copy.deepcopy(response.json())\n",
    "    \n",
    "    response.close()\n",
    "    del response, _url\n",
    "    \n",
    "    for key in _dict:\n",
    "        print(key)\n",
    "        if isinstance(_dict[key], dict):\n",
    "            for k in _dict[key]:\n",
    "                print(f\"\\t{k}: {_dict[key][k]}\")\n",
    "        elif isinstance(_dict[key], list):\n",
    "            for i in range(0, len(_dict[key])):\n",
    "                for k in _dict[key][i]:\n",
    "                    print(f\"\\t{k}: {_dict[key][i][k]}\")\n",
    "\n",
    "\n",
    "elif inport_response.status_code == 401:\n",
    "    print(\"Authentication NOT successful!\")\n",
    "else:\n",
    "    print(f\"Something is wrong!! Status Code: {response.status_code}\")\n",
    "inport_response.close()\n",
    "del inport_response, inport_url\n",
    "# RESULT \n",
    "# Authentication successful!\n",
    "# {'sessionId': '75c3628752d91a126fcbded22998f3e'}\n",
    "# results\n",
    "# \tlastName: Kennedy\n",
    "# \tfirstName: John\n",
    "# \tmiddleName: F\n",
    "# \tphone: 301-427-8149\n",
    "# \temailAddress: john.f.kennedy@noaa.gov\n",
    "# \taddress: 1315 East-West Highway\n",
    "# \tcity: Silver Spring\n",
    "# \tstateProvince: MD\n",
    "# \tpostalCode: 20910\n",
    "# \tcountry: USA\n",
    "# \tcontactType: Person\n",
    "# \trorMaxLength: 9\n",
    "# \torcIdMaxLength: 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search API Specs ###\n",
    "### Endpoints ###\n",
    "#### /inport/api/search ####\n",
    "#### (GET) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, copy\n",
    "inport_url = \"https://www.fisheries.noaa.gov/inport\"\n",
    "inport_response = requests.get(url=inport_url, auth=(credentials[\"username\"], credentials[\"password\"]))\n",
    "if inport_response.status_code == 200:\n",
    "    print(\"Authentication successful!\")\n",
    "    _url = \"https://www.fisheries.noaa.gov/inport/api/get-session-id\"\n",
    "    session_id_response = requests.post(_url, json = credentials)\n",
    "    session_id_dict = copy.deepcopy(session_id_response.json())\n",
    "    #print(session_id_dict)\n",
    "    session_id_response.close()\n",
    "    del _url, session_id_response       \n",
    "    \n",
    "    session_id_dict[\"keywords\"]    = \"environment\"\n",
    "    session_id_dict[\"org\"]         = \"OST\"\n",
    "    session_id_dict[\"catItemType\"] = \"DS,ENT,PRJ\"\n",
    "    print(session_id_dict)\n",
    "\n",
    "    _url = \"https://www.fisheries.noaa.gov/inport/api/search\"\n",
    "    response = requests.get(_url, params = session_id_dict)        \n",
    "    #print(response.json())\n",
    "    _dict = copy.deepcopy(response.json())\n",
    "    response.close()\n",
    "    del response, _url\n",
    "    \n",
    "    for key in _dict:\n",
    "        print(key)\n",
    "        if isinstance(_dict[key], dict):\n",
    "            for k in _dict[key]:\n",
    "                print(f\"\\t{k}: {_dict[key][k]}\")\n",
    "        elif isinstance(_dict[key], list):\n",
    "            for i in range(0, len(_dict[key])):\n",
    "                for k in _dict[key][i]:\n",
    "                    print(f\"\\t{k}: {_dict[key][i][k]}\")\n",
    "\n",
    "elif inport_response.status_code == 401:\n",
    "    print(\"Authentication NOT successful!\")\n",
    "else:\n",
    "    print(f\"Something is wrong!! Status Code: {response.status_code}\")\n",
    "inport_response.close()\n",
    "del inport_response, inport_url\n",
    "# RESULT \n",
    "# Authentication successful!\n",
    "# {'sessionId': '13a880c7ba85d337a572d92f', 'keywords': 'environment', 'org': 'OST', \n",
    "# 'catItemType': 'DS,ENT,PRJ'}\n",
    "# results\n",
    "# \tcatId: 20784\n",
    "# \tcatItemType: Project\n",
    "# \torg: OST\n",
    "# \ttitle: Large Marine Ecosystems (LME)\n",
    "# \tabstractValue: Large Marine Ecosystems (LME) are ocean regions of 200,000 km2 or greater \n",
    "#   that are defined by ecological criteria, including bathymetry, hydrography, marine \n",
    "#   productivity, and trophically linked populations.  Since 1984, NOAA?s LME Program has \n",
    "#   developed ecosystem management tools, initiated projects that have been funded by partner \n",
    "#   organizations, and provided training for developing country part...\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load InPort XML ###\n",
    "#### /inport/api/load-inport-xml ####\n",
    "#### (POST) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n",
      "XML File: WhaleBeluga_CookInletDPS_20250624_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76655\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleBlue_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76657\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleBowhead_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76659\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleFalseKiller_MainHawaiianIslandsInsularDPS_20250624_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76661\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleFinback_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76663\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleGray_WesternNorthPacificDPS_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76665\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleHumpback_AllDPS_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76667\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleKiller_SouthernResidentDPS_20250624_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76669\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleNorthAtlanticRight_20250627_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "\tsummary: Citation Title\n",
      "\tdetails: A citation title must be supplied for Lineage Source. This Lineage Source record cannot be loaded.\n",
      "\tsourceField: /lineage-sources/lineage-source/citation-title\n",
      "\tdestinationField: Section: Lineage Sources | Field: Citation Title\n",
      "\n",
      "\tsummary: Citation Title\n",
      "\tdetails: A citation title must be supplied for Lineage Source. This Lineage Source record cannot be loaded.\n",
      "\tsourceField: /lineage-sources/lineage-source/citation-title\n",
      "\tdestinationField: Section: Lineage Sources | Field: Citation Title\n",
      "\n",
      "childItems\n",
      "\tcatId: 76671\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleNorthPacificRight_20250630_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76673\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleRices_20250630_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76675\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleSei_20250630_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76677\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleSouthernRight_20250626_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76679\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n",
      "XML File: WhaleSperm_20250630_InPort.xml\n",
      "<Response [200]>\n",
      "catId\n",
      "errors\n",
      "warnings\n",
      "childItems\n",
      "\tcatId: 76681\n",
      "\terrors: []\n",
      "\twarnings: []\n",
      "\tchildItems: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import requests, json, copy, os\n",
    "import os\n",
    "# Change Directory\n",
    "os.chdir(home_folder)\n",
    "import user_credentials\n",
    "credentials = user_credentials.main()\n",
    "\n",
    "# InPort URL\n",
    "#inport_url = \"https://test-www.fisheries.noaa.gov/inport\"\n",
    "inport_url = \"https://www.fisheries.noaa.gov/inport\"\n",
    "\n",
    "# Use requests get method to log into InPort\n",
    "inport_response = requests.get(url=inport_url, auth=(credentials[\"username\"], credentials[\"password\"]))\n",
    "\n",
    "# Test if successful i.e. status code 200\n",
    "if inport_response.status_code == 200:\n",
    "    print(\"Authentication successful!\")\n",
    "    \n",
    "    #xml = rf\"{home_folder}\\Metadata Export\\PolygonExample_InPort.xml\"\n",
    "    #xml = rf\"{home_folder}\\Metadata Export\\AbaloneBlack_20250521_InPort.xml\"\n",
    "\n",
    "    #xmls = [rf\"{home_folder}\\Metadata Export\\AbaloneWhite_20250529_InPort.xml\", rf\"{home_folder}\\Metadata Export\\All_Species_Ranges_20250710_InPort.xml\"]\n",
    "    xmls = [rf\"{home_folder}\\Metadata Export\\{xml}\" for xml in os.listdir(rf\"{home_folder}\\Metadata Export\") if xml.endswith(\"_InPort.xml\")]\n",
    "        \n",
    "    for xml in xmls:\n",
    "        print(f\"XML File: {os.path.basename(xml)}\")\n",
    "\n",
    "        # InPort URL to get a session id\n",
    "        #_url = \"https://test-www.fisheries.noaa.gov/inport/api/get-session-id\"\n",
    "        _url = \"https://www.fisheries.noaa.gov/inport/api/get-session-id\"\n",
    "        \n",
    "        # Use requests post method to get a session id for credentials pasted to InPort\n",
    "        session_id_response = requests.post(_url, json = credentials)\n",
    "        \n",
    "        # Make a deep copy of the response, using the method json() to return a Python dictionary\n",
    "        session_id_dict = copy.deepcopy(session_id_response.json())\n",
    "        \n",
    "        # Close the response object and then delete variable\n",
    "        session_id_response.close()\n",
    "        del _url, session_id_response\n",
    "        \n",
    "        # Opening a file for reading\n",
    "        with open(xml, \"r\") as file:\n",
    "            content = file.read()\n",
    "            \n",
    "        # Insert XML content from file into the dictionary, using 'xml' as the key,     \n",
    "        session_id_dict[\"xml\"] = content\n",
    "     \n",
    "        #_url = \"https://test-www.fisheries.noaa.gov/inport/api/load-inport-xml\"\n",
    "        _url = \"https://www.fisheries.noaa.gov/inport/api/load-inport-xml\"\n",
    "        # headers = {\"Content-Type\": \"application/json\"}     \n",
    "        # response = requests.post(url, json=payload, headers=headers)\n",
    "        # response = requests.post(_url, json=session_id_dict, headers=headers)\n",
    "        # Use request post method to load content to InPort\n",
    "        response = requests.post(_url, json=session_id_dict)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(response)\n",
    "            _dict = copy.deepcopy(response.json())\n",
    "            response.close()\n",
    "        \n",
    "            for key in _dict:\n",
    "                print(key)\n",
    "                if isinstance(_dict[key], dict):\n",
    "                    for k in _dict[key]:\n",
    "                        print(f\"\\t{k}: {_dict[key][k]}\")\n",
    "                elif isinstance(_dict[key], list):\n",
    "                    for i in range(0, len(_dict[key])):\n",
    "                        for k in _dict[key][i]:\n",
    "                            print(f\"\\t{k}: {_dict[key][i][k]}\")\n",
    "                        print()\n",
    "  \n",
    "        elif response.status_code == 400:\n",
    "            print(session_id_dict)    \n",
    "            print(response.content)        \n",
    "        \n",
    "        else:\n",
    "            print(f\"Something is wrong with load-inport-xml!!\\n\\tStatus Code: {response.status_code}\\n\\tContent: {response.content}\")\n",
    "    \n",
    "        del response, _url\n",
    "\n",
    "elif inport_response.status_code == 401:\n",
    "    print(\"Authentication NOT successful!\")\n",
    "else:\n",
    "    print(f\"Something is wrong!! Status Code: {response.status_code}\")\n",
    "inport_response.close() \n",
    "del inport_response, inport_url\n",
    "# RESULT ###--->>> Authentication successful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "home_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "print([xml for xml in os.listdir(rf\"{home_folder}\\Metadata Export\") if xml.endswith(\"_InPort.xml\") and xml.startswith(\"Abalone\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
